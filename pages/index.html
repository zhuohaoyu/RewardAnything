---
layout: default
title: "Home"
description: "RewardAnything: Generalizable Principle-Following Reward Models"
---

<!-- Hero Section -->
<section class="gradient-bg text-slate-700 py-12">
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 text-center">
        <div class="mb-6">
            <!-- Use your regular logo - it will look perfect on this lighter background -->
            <img src="{{ '/assets/images/rewardanything-logo-horizontal.png' | relative_url }}" 
                 alt="RewardAnything" 
                 class="h-16 md:h-20 w-auto mx-auto mb-4">
        </div>
        
        <h1 class="text-3xl md:text-4xl font-bold text-slate-800 mb-8 max-w-4xl mx-auto">
            Generalizable Principle-Following Reward Models
        </h1>
        
        <!-- Simplified TLDR with cleaner formatting -->
        <div class="text-lg md:text-xl text-slate-700 max-w-3xl mx-auto leading-relaxed space-y-4 mb-10">
            <p>
                Traditional reward models learn <span class="font-semibold text-slate-900">implicit preferences</span> from fixed data, 
                but human values are <span class="font-semibold text-slate-900">too nuanced</span> for any single, static model.
            </p>
            <p>
                We believe reward models, much like LLMs with instructions, must follow 
                <span class="font-semibold text-blue-700">explicitly specified principles</span>. 
                This unlocks <span class="font-semibold text-slate-900">inference-time adaptability</span> to diverse criteria‚Äî<span class="font-semibold text-slate-900">without costly retraining</span>.
            </p>
        </div>
        
        <!-- Updated CTA Buttons -->
        <div class="flex flex-col sm:flex-row gap-3 justify-center items-center mb-10">
            <a href="{{ site.paper_url | default: '#' }}" 
               class="inline-flex items-center bg-white text-slate-700 px-6 py-2.5 rounded-lg font-semibold hover:bg-slate-50 transition-all transform hover:scale-105 shadow-lg border border-slate-200">
                üìÑ <span class="ml-2">Paper</span>
            </a>
            <a href="{{ site.huggingface_url | default: '#' }}" 
               class="inline-flex items-center bg-gradient-to-r from-yellow-400 to-orange-500 text-white px-6 py-2.5 rounded-lg font-semibold hover:from-yellow-500 hover:to-orange-600 transition-all transform hover:scale-105 shadow-lg">
                ü§ó <span class="ml-2">Model Weights</span>
            </a>
            <a href="#quickstart" 
               class="inline-flex items-center bg-gradient-to-r from-emerald-500 to-teal-600 text-white px-6 py-2.5 rounded-lg font-semibold hover:from-emerald-600 hover:to-teal-700 transition-all transform hover:scale-105 shadow-lg">
                üíª <span class="ml-2">Get Started</span>
            </a>
        </div>
    </div>
</section>

<!-- Authors Section -->
<section class="py-10 bg-white">
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 text-center">
        <div class="text-base md:text-lg text-gray-700 mb-6">
            <div class="space-y-3">
                <!-- First line of authors (5 authors) -->
            <div class="flex flex-wrap justify-center gap-x-6 gap-y-2">
                <span>Zhuohao Yu<sup>1,¬ß</sup></span>
                <span>Jiali Zeng<sup>2</sup></span>
                <span>Weizheng Gu<sup>1</sup></span>
                <span>Yidong Wang<sup>1</sup></span>
                <span>Jindong Wang<sup>3</sup></span>
                </div>
                <!-- Second line of authors (5 authors) -->
                <div class="flex flex-wrap justify-center gap-x-6 gap-y-2">
                <span>Fandong Meng<sup>2</sup></span>
                <span>Jie Zhou<sup>2</sup></span>
                <span>Yue Zhang<sup>4</sup></span>
                <span>Shikun Zhang<sup>1</sup></span>
                <span>Wei Ye<sup>1,‚Ä†</sup></span>
            </div>
        </div>
        </div>
        <div class="text-base md:text-lg text-gray-600">
            <div class="mb-4 font-bold">
                <sup>1</sup>Peking University &emsp;
                <sup>2</sup>WeChat AI &emsp;
                <sup>3</sup>William & Mary &emsp;
                <sup>4</sup>Westlake University
            </div>
            <p class="text-sm md:text-base text-gray-500">
                <sup>¬ß</sup>Work done during internship at WeChat AI &emsp; <sup>‚Ä†</sup>Corresponding author
            </p>
        </div>
    </div>
</section>

<!-- Problem Demonstration with Real Cases -->
<section class="py-20 bg-gradient-to-br from-red-50 to-orange-50">
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
        <div class="text-center mb-16">
            <h2 class="text-3xl md:text-4xl font-bold text-gray-900 mb-4">The Core Problem: Flawed Training & Narrow Evaluation</h2>
            <p class="text-xl text-gray-600 max-w-4xl mx-auto">
                Current reward models face fundamental limitations in how they are <span class="font-semibold text-red-700">trained</span> and <span class="font-semibold text-red-700">evaluated</span>, hindering their ability to truly align with diverse human values.
            </p>
        </div>

        <!-- Problematic Training -->
        <div class="mb-12 bg-white rounded-2xl p-8 shadow-lg border border-red-100">
            <h3 class="text-2xl font-bold text-gray-900 mb-4 text-center">1. Problematic Training: Learning Static & Biased Preferences</h3>
            <p class="text-gray-700 mb-4 text-center max-w-3xl mx-auto">
                Reward models are typically trained on vast datasets of <span class="font-semibold text-red-700">(prompt, chosen response, rejected response)</span> tuples. This teaches the model a <span class="font-semibold text-red-700">single, implicit preference</span> distribution.
            </p>
            <div class="grid md:grid-cols-2 gap-6 text-sm text-gray-600">
                <div class="bg-red-50 p-4 rounded-lg">
                    <h4 class="font-semibold text-red-800 mb-1">No Principled Control:</h4>
                    <p>Even if the prompt and responses are identical, applying <span class="italic">different evaluation principles</span> (e.g., "be concise" vs. "be detailed") should lead to different rankings. Current RMs struggle to adapt this way without costly retraining for each new principle.</p>
                </div>
                <div class="bg-red-50 p-4 rounded-lg">
                    <h4 class="font-semibold text-red-800 mb-1">Implicit & Outcome-Only Learning:</h4>
                    <p>Models learn <span class="italic">what</span> to prefer based on outcomes, but not <span class="italic">why</span>. This lack of explicit rationale leads to learning superficial patterns or <span class="font-semibold text-red-700">spurious correlations</span> (e.g., "longer is better," "lists are good") rather than the true underlying human intent, as shown below.</p>
                </div>
            </div>
        </div>

        <!-- Problem Illustration with Real Cases -->
        <div class="grid lg:grid-cols-2 gap-12 mb-16">
            <!-- Case 1: Length Bias -->
            <div class="bg-white rounded-2xl p-8 shadow-lg border border-red-100">
                <div class="flex items-center mb-6">
                    <div class="bg-red-100 p-3 rounded-lg mr-4">
                        <svg class="w-6 h-6 text-red-600" fill="currentColor" viewBox="0 0 20 20">
                            <path fill-rule="evenodd" d="M18 10a8 8 0 11-16 0 8 8 0 0116 0zm-7 4a1 1 0 11-2 0 1 1 0 012 0zm-1-9a1 1 0 00-1 1v4a1 1 0 102 0V6a1 1 0 00-1-1z" clip-rule="evenodd"/>
                        </svg>
                    </div>
                    <div>
                        <h3 class="text-xl font-bold text-gray-900">Issue #1: Length = Quality Bias</h3>
                        <p class="text-sm text-gray-600">Models learn "longer responses are better" from pairs where correctness correlates with length.</p>
                    </div>
                </div>
                
                <div class="space-y-4">
                    <div class="bg-blue-50 p-4 rounded-lg border-l-4 border-blue-400">
                        <div class="text-sm font-semibold text-blue-900 mb-2">üôã Prompt:</div>
                        <p class="text-blue-800 text-sm">"What are some species of bears that are now extinct?"</p>
                    </div>
                    
                    <div class="grid md:grid-cols-2 gap-4">
                        <div class="bg-green-50 p-4 rounded-lg border border-green-200">
                            <div class="flex items-center mb-2">
                                <span class="text-green-600 font-semibold text-sm mr-2">‚úÖ Chosen (Long + Correct)</span>
                            </div>
                            <p class="text-green-800 text-xs leading-relaxed">
                                "Several species of bears have become extinct... <strong>Cave Bear (Ursus spelaeus)</strong>: One of the best-known extinct bear species... <strong>Short-faced Bear (Arctodus simus)</strong>: Once the largest..."
                            </p>
                            <div class="mt-2 text-xs text-green-600">
                                ‚úì Accurate facts ‚úì Detailed explanations
                            </div>
                        </div>
                        
                        <div class="bg-red-50 p-4 rounded-lg border border-red-200">
                            <div class="flex items-center mb-2">
                                <span class="text-red-600 font-semibold text-sm mr-2">‚ùå Rejected (Short + Wrong)</span>
                            </div>
                            <p class="text-red-800 text-xs leading-relaxed">
                                "Three species of bears that are now extinct are the <strong>woolly mammoth</strong>, the <strong>woolly rhinoceros</strong>, and the <strong>thylacine</strong>."
                            </p>
                            <div class="mt-2 text-xs text-red-600">
                                ‚ùå Factually incorrect ‚ùå None are bears
                            </div>
                        </div>
                    </div>
                    
                    <div class="bg-yellow-50 p-4 rounded-lg border border-yellow-200">
                        <div class="text-sm font-semibold text-yellow-800 mb-1">‚ö†Ô∏è What Current RMs Learn:</div>
                        <p class="text-yellow-700 text-sm">A spurious correlation: "Longer responses are better." This preference is static, but what if the user actually preferred a brief, accurate answer?</p>
                    </div>
                </div>
            </div>

            <!-- Case 2: Format Over Substance -->
            <div class="bg-white rounded-2xl p-8 shadow-lg border border-red-100">
                <div class="flex items-center mb-6">
                    <div class="bg-red-100 p-3 rounded-lg mr-4">
                        <svg class="w-6 h-6 text-red-600" fill="currentColor" viewBox="0 0 20 20">
                            <path fill-rule="evenodd" d="M18 10a8 8 0 11-16 0 8 8 0 0116 0zm-7 4a1 1 0 11-2 0 1 1 0 012 0zm-1-9a1 1 0 00-1 1v4a1 1 0 102 0V6a1 1 0 00-1-1z" clip-rule="evenodd"/>
                        </svg>
                    </div>
                    <div>
                        <h3 class="text-xl font-bold text-gray-900">Issue #2: Format Over Substance</h3>
                        <p class="text-sm text-gray-600">Models often prioritize familiar structures (e.g., lists) over equally valid, natural content.</p>
                    </div>
                </div>
                
                <div class="space-y-4">
                    <div class="bg-blue-50 p-4 rounded-lg border-l-4 border-blue-400">
                        <div class="text-sm font-semibold text-blue-900 mb-2">üôã Prompt:</div>
                        <p class="text-blue-800 text-sm">"What are some good browser alternatives to Chrome?"</p>
                    </div>
                    
                    <div class="grid md:grid-cols-2 gap-4">
                        <div class="bg-green-50 p-4 rounded-lg border border-green-200">
                            <div class="flex items-center mb-2">
                                <span class="text-green-600 font-semibold text-sm mr-2">‚úÖ Chosen (Well-Structured)</span>
                            </div>
                            <p class="text-green-800 text-xs leading-relaxed">
                                "There are several good browser alternatives to Chrome:
                                <br><strong>1. Mozilla Firefox:</strong> Known for strong privacy features, being open-source, and highly customizable.
                                <br><strong>2. Microsoft Edge:</strong> Now built on Chromium, offering good performance and compatibility."
                            </p>
                            <div class="mt-2 text-xs text-green-600">
                                ‚úì Clear, itemized structure ‚úì Detailed points
                            </div>
                        </div>
                        
                        <div class="bg-orange-50 p-4 rounded-lg border border-orange-200">
                            <div class="flex items-center mb-2">
                                <span class="text-orange-600 font-semibold text-sm mr-2">‚ö†Ô∏è Rejected (Natural but Correct)</span>
                            </div>
                            <p class="text-orange-800 text-xs leading-relaxed">
                                "Sure! For browser alternatives, you could check out Firefox ‚Äì it's really good for privacy and you can customize it a lot. Microsoft Edge is another option; it's pretty fast now that it uses Chromium tech."
                            </p>
                            <div class="mt-2 text-xs text-orange-600">
                                ‚úì Factually correct ‚úì Conversational style ‚úì Complete
                            </div>
                        </div>
                    </div>
                    
                    <div class="bg-yellow-50 p-4 rounded-lg border border-yellow-200">
                        <div class="text-sm font-semibold text-yellow-800 mb-1">‚ö†Ô∏è What Current RMs Learn:</div>
                        <p class="text-yellow-700 text-sm">"Structured, list-like responses are better." This overlooks that a natural, conversational style might be equally informative or even preferred by some users.</p>
                    </div>
                </div>
            </div>
        </div>

        <!-- Incomplete Evaluation -->
        <div class="my-16 bg-white rounded-2xl p-8 shadow-lg border border-orange-100">
            <h3 class="text-2xl font-bold text-gray-900 mb-4 text-center">2. Incomplete Evaluation: Missing True Generalization</h3>
            <p class="text-gray-700 mb-4 text-center max-w-3xl mx-auto">
                Existing Reward Model benchmarks primarily measure how well an RM aligns with a <span class="font-semibold text-orange-700">single, predefined preference distribution</span> (often the one it was trained on or a similar one).
            </p>
            <div class="grid md:grid-cols-2 gap-6 text-sm text-gray-600">
                <div class="bg-orange-50 p-4 rounded-lg">
                    <h4 class="font-semibold text-orange-800 mb-1">Ignoring Multifaceted Values:</h4>
                    <p>Human preferences are complex, context-dependent, and multifaceted. A truly useful RM must adapt to <span class="italic">any</span> explicitly stated principle, not just echo a single, baked-in preference.</p>
                </div>
                <div class="bg-orange-50 p-4 rounded-lg">
                    <h4 class="font-semibold text-orange-800 mb-1">Superficial Alignment:</h4>
                    <p>This narrow evaluation fails to assess the critical capability of <span class="font-semibold text-orange-700">generalizing to diverse and novel principles</span> at inference time, which is essential for robust and trustworthy AI systems.</p>
                </div>
            </div>
        </div>

        <!-- The Core Problems -->
        <div class="bg-white rounded-2xl p-8 shadow-lg border border-gray-200">
            <h3 class="text-2xl font-bold text-gray-900 mb-2 text-center">Consequences: Why Current Reward Models Fall Short</h3>
            <p class="text-gray-600 text-center mb-8 max-w-3xl mx-auto">These fundamental issues in training and evaluation lead to several critical shortcomings:</p>
            
            <div class="grid md:grid-cols-2 lg:grid-cols-4 gap-6">
                <div class="text-center">
                    <div class="bg-red-100 w-16 h-16 rounded-full flex items-center justify-center mx-auto mb-4">
                        <!-- Icon for Overfitting to Static Preferences -->
                        <svg class="w-8 h-8 text-red-600" fill="currentColor" viewBox="0 0 20 20">
                            <path fill-rule="evenodd" d="M5 9V7a5 5 0 0110 0v2a2 2 0 012 2v5a2 2 0 01-2 2H5a2 2 0 01-2-2v-5a2 2 0 012-2zm5-3a3 3 0 00-3 3v2h6V7a3 3 0 00-3-3z" clip-rule="evenodd" />
                        </svg>
                    </div>
                    <h4 class="font-semibold text-gray-900 mb-2">Overfitting to Static Preferences</h4>
                    <p class="text-sm text-gray-600">RMs master a single, fixed preference from training data, failing to grasp the multifaceted nature of human values or adapt to diverse contexts.</p>
                </div>
                
                <div class="text-center">
                    <div class="bg-red-100 w-16 h-16 rounded-full flex items-center justify-center mx-auto mb-4">
                        <!-- Icon for Opaque & Implicit Reasoning -->
                        <svg class="w-8 h-8 text-red-600" fill="currentColor" viewBox="0 0 20 20">
                            <path fill-rule="evenodd" d="M18 10a8 8 0 11-16 0 8 8 0 0116 0zm-8-3a1 1 0 00-.867.5 1 1 0 11-1.731-1A3 3 0 0113 8a3.001 3.001 0 01-2 2.83V11a1 1 0 11-2 0v-1a1 1 0 011-1 1 1 0 100-2zm0 8a1 1 0 100-2 1 1 0 000 2z" clip-rule="evenodd" />
                        </svg>
                    </div>
                    <h4 class="font-semibold text-gray-900 mb-2">Opaque & Implicit Reasoning</h4>
                    <p class="text-sm text-gray-600">Learning from outcomes alone (chosen/rejected pairs), RMs lack an explicit understanding of *why* a response is preferred, making their judgments uninterpretable black boxes.</p>
                </div>
                
                <div class="text-center">
                    <div class="bg-red-100 w-16 h-16 rounded-full flex items-center justify-center mx-auto mb-4">
                        <!-- Icon for Vulnerability to Spurious Correlations -->
                        <svg class="w-8 h-8 text-red-600" fill="currentColor" viewBox="0 0 20 20">
                            <path fill-rule="evenodd" d="M8.485 2.495c.673-1.167 2.357-1.167 3.03 0l6.28 10.875c.673 1.167-.17 2.625-1.516 2.625H3.72c-1.347 0-2.189-1.458-1.515-2.625L8.485 2.495zM10 5a.75.75 0 01.75.75v3.5a.75.75 0 01-1.5 0v-3.5A.75.75 0 0110 5zm0 9a1 1 0 100-2 1 1 0 000 2z" clip-rule="evenodd" />
                        </svg>
                    </div>
                    <h4 class="font-semibold text-gray-900 mb-2">Vulnerability to Spurious Correlations</h4>
                    <p class="text-sm text-gray-600">Implicit learning on biased data leads RMs to mistakenly learn superficial cues (e.g., length, format, specific keywords) as proxies for genuine quality.</p>
                </div>
                
                <div class="text-center">
                    <div class="bg-red-100 w-16 h-16 rounded-full flex items-center justify-center mx-auto mb-4">
                        <!-- Icon for Costly & Inefficient Adaptation -->
                        <svg class="w-8 h-8 text-red-600" fill="currentColor" viewBox="0 0 20 20">
                           <path fill-rule="evenodd" d="M10 18a8 8 0 100-16 8 8 0 000 16zm.75-13a.75.75 0 00-1.5 0v5c0 .414.336.75.75.75h4a.75.75 0 000-1.5h-3.25V5z" clip-rule="evenodd" />
                        </svg>
                    </div>
                    <h4 class="font-semibold text-gray-900 mb-2">Costly & Inefficient Adaptation</h4>
                    <p class="text-sm text-gray-600">Due to overfit, static preferences and opaque reasoning, aligning RMs with new criteria or principles demands expensive data collection and full retraining cycles.</p>
                </div>
            </div>
        </div>
    </div>
</section>

<!-- New Solution Section -->
<section class="py-20 bg-slate-50"> <!-- Neutral background for the solution section -->
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
        <div class="text-center mb-16">
            <h2 class="text-3xl md:text-4xl font-bold text-gray-900 mb-4">The Solution: Principle-Following Reward Models</h2>
            <p class="text-xl text-gray-600 max-w-4xl mx-auto">
                To overcome these limitations, we propose a paradigm shift towards reward models that explicitly understand and follow natural language principles. This approach enables dynamic adaptation to any evaluation criteria without costly retraining and is embodied by two key innovations:
            </p>
        </div>

        <div class="grid md:grid-cols-1 lg:grid-cols-2 gap-10 mb-16 items-stretch">
            <!-- Sub-block 1: Evaluating Principle-Following (RABench) -->
            <div class="bg-white p-8 rounded-xl shadow-lg border border-gray-200 flex flex-col">
                <div class="flex items-center mb-4">
                    <span class="text-3xl mr-3">üéØ</span>
                    <h3 class="text-2xl font-bold text-blue-700">1. A New Evaluation Paradigm: RABench</h3>
                </div>
                <p class="text-gray-700 mb-4 flex-grow">
                    Current benchmarks assess how well RMs fit a <span class="font-semibold">single, fixed preference</span>. This is insufficient. We argue that, analogous to how Large Language Models (LLMs) are valued for their ability to <span class="font-semibold text-blue-600">follow diverse instructions</span>, reward models must be evaluated on their capacity to <span class="font-semibold text-blue-600">follow diverse principles</span>.
                </p>
                <p class="text-gray-700 mb-4 flex-grow">
                    To this end, we introduce <strong class="text-blue-700">RABench (RewardAnything Benchmark)</strong>. It is a comprehensive benchmark meticulously designed to assess the principle-following capabilities of RMs across various domains (chat, code, safety, math) and a wide array of explicit natural language criteria.
                </p>
                 <p class="text-gray-700">
                    RABench moves beyond static preference matching, pushing for RMs that demonstrate true generalization in understanding and applying "goodness" based on varied, explicit guidance.
                </p>
            </div>

            <!-- Sub-block 2: RewardAnything Model -->
            <div class="bg-white p-8 rounded-xl shadow-lg border border-gray-200 flex flex-col">
                 <div class="flex items-center mb-4">
                    <span class="text-3xl mr-3">üèÜ</span>
                    <h3 class="text-2xl font-bold text-green-700">2. The RewardAnything Model</h3>
                </div>
                <p class="text-gray-700 mb-4 flex-grow">
                    We develop <strong class="text-green-700">RewardAnything</strong>, a novel reward model engineered to embody this principle-following paradigm.
                </p>
                <p class="text-gray-700 mb-4 flex-grow">
                    Trained using advanced Reinforcement Learning (RL) techniques on principle-conditioned preference data, RewardAnything learns to robustly distinguish better responses from worse ones by <span class="font-semibold text-green-600">directly conditioning on explicit natural language principles</span> provided at inference time. This allows it to adapt its judgment dynamically without any retraining.
                </p>
                <p class="text-gray-700">
                    A key feature is its <span class="font-semibold text-green-600">inference-time reasoning process</span>. RewardAnything not only scores responses according to the given principle but can also articulate an explanation for its judgment, enhancing transparency and trustworthiness.
                </p>
            </div>
        </div>

        <!-- Figure 1 Moved Here -->
        <div class="paper-figure-container mb-0"> <!-- paper-figure-container already has good styling -->
            <div class="max-w-5xl mx-auto">
                <img src="{{ '/assets/images/figure_1_placeholder.jpg' | relative_url }}" 
                     alt="Figure 1: Current post-training optimization paradigm vs RewardAnything approach"
                     class="w-full h-auto rounded-lg shadow-sm">
            </div>
        </div>
    </div>
</section>

<!-- Refer to Paper for Technical Details -->
<section class="py-16 bg-slate-100">
    <div class="max-w-4xl mx-auto px-4 sm:px-6 lg:px-8 text-center">
        <h2 class="text-2xl md:text-3xl font-semibold text-gray-800 mb-6">
            <span class="mr-2">üìñ</span>Dive Deeper into the Details
        </h2>
        <p class="text-lg md:text-xl text-gray-700 leading-relaxed mb-8">
            For a comprehensive understanding of our methodology, technical innovations, detailed model architecture, training procedures, and full experimental setup, please refer to our full research paper. The paper provides an in-depth exploration of the concepts presented here.
        </p>
        <a href="{{ site.paper_url | default: '#' }}" 
           class="inline-flex items-center bg-blue-600 text-white px-8 py-3 rounded-lg font-semibold hover:bg-blue-700 transition-all transform hover:scale-105 shadow-lg text-base md:text-lg">
            üìÑ <span class="ml-2">Read the Full Paper</span>
        </a>
    </div>
</section>

<!-- Installation & Usage Guide -->
<section id="quickstart" class="py-20 bg-white">
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
        <div class="text-center mb-16">
            <h2 class="text-3xl md:text-4xl font-bold text-gray-900 mb-4">üöÄ Quick Start</h2>
            <p class="text-xl text-gray-600 max-w-4xl mx-auto">
                RewardAnything offers three flexible deployment options to fit your workflow, from quick experimentation to production-scale evaluation.
            </p>
        </div>

            <!-- Installation -->
        <div class="mb-16">
            <div class="bg-gray-50 rounded-xl p-8">
                <h3 class="text-xl font-bold text-gray-900 mb-4">üì¶ Installation</h3>
                <div class="bg-gray-900 rounded-lg p-4">
                    <code class="text-green-400 font-mono">pip install rewardanything</code>
                </div>
            </div>
        </div>

        <!-- Three Deployment Methods -->
        <div class="grid md:grid-cols-3 gap-8 mb-16">
            <!-- Method 1: Local Inference -->
            <div class="deployment-card blue bg-gradient-to-br from-blue-50 to-indigo-100 rounded-xl p-6 border border-blue-200" 
                 onclick="selectDeploymentMethod('local')" data-method="local">
                <!-- Recommended Tag -->
                <div class="flex justify-between items-start mb-3">
                    <span class="inline-block bg-blue-500 text-white text-xs px-2 py-1 rounded-full font-medium">
                        Recommended for Beginners
                    </span>
                    <span class="text-2xl">üè†</span>
                </div>
                
                <h3 class="text-xl font-bold text-gray-900 mb-2">Local Inference</h3>
                <p class="text-gray-600 mb-4">Perfect for quick experimentation and research</p>
                
                <!-- Use Case Tags -->
                <div class="flex flex-wrap gap-2 mb-4">
                    <span class="inline-block bg-blue-100 text-blue-800 text-xs px-2 py-1 rounded-md font-medium">Quick Testing</span>
                    <span class="inline-block bg-blue-100 text-blue-800 text-xs px-2 py-1 rounded-md font-medium">Research</span>
                    <span class="inline-block bg-blue-100 text-blue-800 text-xs px-2 py-1 rounded-md font-medium">Offline Use</span>
                </div>
                
                <div class="space-y-3 mb-4">
                    <div>
                        <div class="text-sm font-semibold text-green-700 mb-1">‚úÖ Pros:</div>
                        <ul class="text-xs text-gray-600 space-y-1">
                            <li>‚Ä¢ Simple one-line setup</li>
                            <li>‚Ä¢ No external dependencies</li>
                            <li>‚Ä¢ Full control & offline capable</li>
                        </ul>
                    </div>
                    
                    <div>
                        <div class="text-sm font-semibold text-orange-700 mb-1">‚ö†Ô∏è Cons:</div>
                        <ul class="text-xs text-gray-600 space-y-1">
                            <li>‚Ä¢ Local GPU required (8GB+ VRAM)</li>
                            <li>‚Ä¢ Not ideal for batch processing</li>
                        </ul>
                    </div>
                </div>
                
                <div class="flex justify-center">
                    <div class="text-blue-600 text-sm font-medium bg-blue-100 px-4 py-2 rounded-lg hover:bg-blue-200 transition-colors cursor-pointer">
                        View Guides ‚Üí
                    </div>
                </div>
            </div>

            <!-- Method 2: vLLM Deployment -->
            <div class="deployment-card emerald bg-gradient-to-br from-emerald-50 to-green-100 rounded-xl p-6 border border-emerald-200" 
                 onclick="selectDeploymentMethod('vllm')" data-method="vllm">
                <!-- Recommended Tag -->
                <div class="flex justify-between items-start mb-3">
                    <span class="inline-block bg-emerald-500 text-white text-xs px-2 py-1 rounded-full font-medium">
                        Recommended for Production
                    </span>
                    <span class="text-2xl">üöÄ</span>
                </div>
                
                <h3 class="text-xl font-bold text-gray-900 mb-2">vLLM Deployment</h3>
                <p class="text-gray-600 mb-4">Optimized for high-throughput and production</p>
                
                <!-- Use Case Tags -->
                <div class="flex flex-wrap gap-2 mb-4">
                    <span class="inline-block bg-emerald-100 text-emerald-800 text-xs px-2 py-1 rounded-md font-medium">Production</span>
                    <span class="inline-block bg-emerald-100 text-emerald-800 text-xs px-2 py-1 rounded-md font-medium">RLHF Training</span>
                    <span class="inline-block bg-emerald-100 text-emerald-800 text-xs px-2 py-1 rounded-md font-medium">Batch Processing</span>
                </div>
                
                <div class="space-y-3 mb-4">
                    <div>
                        <div class="text-sm font-semibold text-green-700 mb-1">‚úÖ Pros:</div>
                        <ul class="text-xs text-gray-600 space-y-1">
                            <li>‚Ä¢ Distributed & concurrent inference</li>
                            <li>‚Ä¢ Production-ready scalability</li>
                            <li>‚Ä¢ Optimized memory usage</li>
                        </ul>
                    </div>
                    
                    <div>
                        <div class="text-sm font-semibold text-orange-700 mb-1">‚ö†Ô∏è Cons:</div>
                        <ul class="text-xs text-gray-600 space-y-1">
                            <li>‚Ä¢ vLLM setup required</li>
                            <li>‚Ä¢ More complex configuration</li>
                        </ul>
                    </div>
                </div>
                
                <div class="flex justify-center">
                    <div class="text-emerald-600 text-sm font-medium bg-emerald-100 px-4 py-2 rounded-lg hover:bg-emerald-200 transition-colors cursor-pointer">
                        View Guides ‚Üí
                    </div>
                </div>
            </div>

            <!-- Method 3: Transformers Direct -->
            <div class="deployment-card purple bg-gradient-to-br from-purple-50 to-violet-100 rounded-xl p-6 border border-purple-200" 
                 onclick="selectDeploymentMethod('huggingface')" data-method="huggingface">
                <!-- Recommended Tag -->
                <div class="flex justify-between items-start mb-3">
                    <span class="inline-block bg-purple-500 text-white text-xs px-2 py-1 rounded-full font-medium">
                        Recommended for Customization
                    </span>
                    <span class="text-2xl">üîß</span>
                </div>
                
                <h3 class="text-xl font-bold text-gray-900 mb-2">Transformers Direct</h3>
                <p class="text-gray-600 mb-4">Maximum flexibility for custom workflows</p>
                
                <!-- Use Case Tags -->
                <div class="flex flex-wrap gap-2 mb-4">
                    <span class="inline-block bg-purple-100 text-purple-800 text-xs px-2 py-1 rounded-md font-medium">Custom Logic</span>
                    <span class="inline-block bg-purple-100 text-purple-800 text-xs px-2 py-1 rounded-md font-medium">Research</span>
                    <span class="inline-block bg-purple-100 text-purple-800 text-xs px-2 py-1 rounded-md font-medium">Low-level Control</span>
                </div>
                
                <div class="space-y-3 mb-4">
                    <div>
                        <div class="text-sm font-semibold text-green-700 mb-1">‚úÖ Pros:</div>
                        <ul class="text-xs text-gray-600 space-y-1">
                            <li>‚Ä¢ Full model control & access</li>
                            <li>‚Ä¢ Custom processing pipelines</li>
                            <li>‚Ä¢ HuggingFace ecosystem</li>
                        </ul>
                    </div>
                    
                    <div>
                        <div class="text-sm font-semibold text-orange-700 mb-1">‚ö†Ô∏è Cons:</div>
                        <ul class="text-xs text-gray-600 space-y-1">
                            <li>‚Ä¢ Manual output parsing</li>
                            <li>‚Ä¢ More boilerplate code</li>
                        </ul>
                    </div>
                </div>
                
                <div class="flex justify-center">
                    <div class="text-purple-600 text-sm font-medium bg-purple-100 px-4 py-2 rounded-lg hover:bg-purple-200 transition-colors cursor-pointer">
                        View Guides ‚Üí
                    </div>
                </div>
            </div>
        </div>

        <!-- Code Examples -->
        <div class="bg-gray-50 rounded-xl p-8">
            <!-- Local Inference Tab -->
            <div id="local-tab" class="tab-content">
                <div class="flex items-center gap-3 mb-6">
                    <span class="text-2xl">üè†</span>
                    <div>
                        <h4 class="text-lg font-bold text-gray-900">Local Inference</h4>
                        <p class="text-sm text-gray-600">Simple setup for quick testing and research</p>
                    </div>
                </div>
                <div class="rounded-lg overflow-hidden">
                    <pre><code class="language-python">import rewardanything

# Load model locally (similar to HuggingFace)
reward_model = rewardanything.from_pretrained("WisdomShell/RewardAnything-8B-v1", device="cuda")

# Get comprehensive evaluation
result = reward_model.judge(
    principle="I prefer clear, concise and helpful responses over long and detailed ones.",
    prompt="How do I learn Python programming effectively?", 
    responses={ # responses with keys, note these are masked and shuffled and then given to RewardAnything to prevent cheating
        "response_a": "Start with Python.org\\'s tutorial, practice daily with small projects, and join r/learnpython for help. Focus on fundamentals first.",
        "response_b": "Here\\'s a comprehensive approach: 1) Start with Python basics including variables, data types, operators, control structures like if-statements, for-loops, while-loops, and functions, 2) Practice with small projects like calculators, text games, and data manipulation scripts, 3) Use interactive platforms like Codecademy, Python.org\\'s official tutorial, edX courses, Coursera specializations, and YouTube channels, 4) Join communities like r/learnpython, Stack Overflow, Python Discord servers, and local meetups for support and networking, 5) Build progressively complex projects including web scrapers, APIs, data analysis tools, and web applications, 6) Read books like \\'Automate the Boring Stuff\\', \\'Python Crash Course\\', and \\'Effective Python\\', 7) Dedicate 1-2 hours daily for consistent progress and track your learning journey.",
        "response_c": "Learn Python by coding."
    }
)

# Access results
print(f"Scores: {result.scores}")
print(f"Ranking: {result.ranking}")
print(f"Reasoning: {result.reasoning}")</code></pre>
                </div>
            </div>

            <!-- vLLM Deployment Tab -->
            <div id="vllm-tab" class="tab-content hidden">
                <div class="flex items-center gap-3 mb-6">
                    <span class="text-2xl">üöÄ</span>
                    <div>
                        <h4 class="text-lg font-bold text-gray-900">vLLM Deployment</h4>
                        <p class="text-sm text-gray-600">Production-ready setup for high-throughput evaluation</p>
                    </div>
                </div>
                <div class="space-y-6">
                    <div>
                        <h5 class="font-semibold text-gray-800 mb-2">Step 1: Setup vLLM Server</h5>
                        <div class="rounded-lg overflow-hidden">
                            <pre><code class="language-bash"># Start vLLM server with RewardAnything model
vllm serve WisdomShell/RewardAnything-8B-v1 --host 0.0.0.0 --port 8000</code></pre>
                        </div>
                    </div>

                    <div>
                        <h5 class="font-semibold text-gray-800 mb-2">Step 2: Start RewardAnything Server</h5>
                        <div class="rounded-lg overflow-hidden">
                            <pre><code class="language-bash"># Start the RewardAnything API server
rewardanything serve -c config.json --port 8001</code></pre>
                        </div>
                    </div>

                    <div>
                        <h5 class="font-semibold text-gray-800 mb-2">Step 3: Use in Your Code</h5>
                        <div class="rounded-lg overflow-hidden">
                            <pre><code class="language-python">import rewardanything

# Connect to the RewardAnything server
client = rewardanything.Client("http://localhost:8001")

# Process batch requests efficiently
requests = [
    {
        "principle": "Prefer clear, concise and helpful responses...",
        "prompt": "How to learn programming?",
        "responses": {
            "assistant_a": "Start with Python, practice daily, build projects.",
            "assistant_b": "Read books and hope for the best.",
            "assistant_c": "Start with Python.org's tutorial, practice daily..."
        }
    },
    # ... more requests
]

results = client.judge_batch(requests)
for result in results:
    print(f"Winner: {result.ranking[0]}")</code></pre>
                        </div>
                    </div>
                </div>
            </div>

            <!-- HuggingFace Direct Tab -->
            <div id="huggingface-tab" class="tab-content hidden">
                <div class="flex items-center gap-3 mb-6">
                    <span class="text-2xl">üîß</span>
                    <div>
                        <h4 class="text-lg font-bold text-gray-900">Direct Transformers Integration</h4>
                        <p class="text-sm text-gray-600">Maximum flexibility for custom workflows and advanced use cases</p>
                    </div>
                </div>
                <div class="rounded-lg overflow-hidden">
                    <pre><code class="language-python">from transformers import AutoTokenizer, AutoModelForCausalLM
from rewardanything.processing import prepare_chat_messages, parse_rewardanything_output
import torch

# Load model and tokenizer directly
model = AutoModelForCausalLM.from_pretrained(
    "WisdomShell/RewardAnything-8B-v1",
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained("WisdomShell/RewardAnything-8B-v1")

# Prepare evaluation data
principle = "Judge responses based on helpfulness and accuracy"
prompt = "What is the capital of France?"
responses = {
    "model_a": "Paris is the capital of France.",
    "model_b": "I think it might be Lyon or Paris."
}

# Prepare chat messages (handles masking automatically)
messages, masked2real = prepare_chat_messages(principle, prompt, responses)

# Format with chat template
formatted_input = tokenizer.apply_chat_template(
    messages, tokenize=False, add_generation_prompt=True
)

# Generate response
inputs = tokenizer(formatted_input, return_tensors="pt").to(model.device)
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=4096,
        temperature=0.1,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )

# Parse structured results (handles JSON parsing robustly)
output_text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
result = parse_rewardanything_output(output_text, masked2real)

print(f"Parsed scores: {result.scores}")
print(f"Ranking: {result.ranking}")</code></pre>
                </div>
            </div>
        </div>
    </div>
</section>

<!-- Advanced Usage -->
<section class="py-20 bg-gray-50">
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
        <div class="text-center mb-16">
            <h2 class="text-3xl md:text-4xl font-bold text-gray-900 mb-4">üî¨ Advanced Usage</h2>
            <p class="text-xl text-gray-600 max-w-4xl mx-auto">
                Unlock the full potential of RewardAnything by leveraging sophisticated principles and seamlessly integrating it into your RLHF workflows.
            </p>
        </div>

        <div class="grid md:grid-cols-2 gap-8 items-stretch"> <!-- items-stretch to make cards same height if content varies -->
            <!-- Complex Principles -->
            <div class="bg-white rounded-xl p-8 shadow-lg border border-gray-200 flex flex-col">
                <div class="flex items-center mb-4">
                    <span class="text-2xl mr-3">üß©</span>
                    <h3 class="text-xl lg:text-2xl font-bold text-gray-900">Complex Principles</h3>
                </div>
                <p class="text-gray-700 mb-4 leading-relaxed flex-grow">
                    RewardAnything excels when provided with clear, structured principles, especially for nuanced tasks involving multiple, potentially conflicting objectives. Define criteria, assign weights (e.g., via textual emphasis or explicit percentages), and specify priorities to guide the model's judgment effectively. This allows for fine-grained control over the evaluation process.
                </p>
                <div class="rounded-lg overflow-hidden bg-gray-800 mt-auto">
                    <pre class="p-4"><code class="language-python text-sm"># Define a detailed, multi-faceted principle
complex_principle = """
Safety comes first but also be sure not to encourage
overly sensitive reiections for safe or benignly
borderline queries. Next, equally value warmth,
appropriate humor (to deflect borderline harm), 
and genuine helpfulness. Remember, content and tone 
are more important than presentation style.

"""

# Assume 'reward_model' is initialized
# prompt = "Your specific prompt here"
# responses = {"res_a": "...", "res_b": "..."}
result = reward_model.judge(
    principle=complex_principle,
    prompt=prompt,
    responses=responses
)</code></pre>
                </div>
            </div>

            <!-- RLHF Integration -->
            <div class="bg-white rounded-xl p-8 shadow-lg border border-gray-200 flex flex-col">
                <div class="flex items-center mb-4">
                    <span class="text-2xl mr-3">üîÑ</span>
                    <h3 class="text-xl lg:text-2xl font-bold text-gray-900">RLHF Integration</h3>
                </div>
                <p class="text-gray-700 mb-4 leading-relaxed flex-grow">
                    Seamlessly integrate RewardAnything into your Reinforcement Learning from Human Feedback (RLHF) pipelines. It can serve as a dynamic, principle-driven reward function. RewardAnything is compatible with popular RL frameworks (e.g., TRL, veRL), allowing you to guide model generation based on explicit criteria rather than static preferences.
                </p>
                <p class="text-gray-700 mb-4 leading-relaxed flex-grow">
                    Detailed integration examples and best practices can be found in our official repository.
                </p>
                <div class="rounded-lg overflow-hidden bg-gray-800 mt-auto">
                    <pre class="p-4"><code class="language-python text-sm"># Example: Use in a PPO-style training loop
# Assume 'reward_model' is initialized
# principle = "Your guiding principle"
# prompt = "The input prompt"

def reward_function(principle, prompt, response_text):
    eval_responses = {"generated": response_text}
    result = reward_model.judge(
        principle=principle,
        prompt=prompt,
        responses=eval_responses
    )
    return result.scores.get("generated", 0.0)

# generated_responses = ["response1", "response2", ...]
rewards = [reward_function(principle, prompt, resp) 
           for resp in generated_responses]</code></pre>
                </div>
            </div>
        </div>
    </div>
</section>

<!-- Performance Results -->
<section class="py-20 bg-gradient-to-br from-blue-50 to-indigo-100">
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
        <div class="text-center mb-16">
            <h2 class="text-3xl md:text-4xl font-bold text-gray-900 mb-4">State-of-the-Art Performance</h2>
            <p class="text-xl text-gray-600 max-w-4xl mx-auto">
                RewardAnything achieves excellent performance on both traditional benchmarks and our new principle-following evaluation. Below are highlights from RM-Bench and our proposed RABench. For full details, additional benchmarks, and ablation studies, please see our paper.
            </p>
        </div>

        <div class="space-y-12 md:space-y-16">
            <div class="bg-white p-4 md:p-6 rounded-xl shadow-xl border border-gray-200">
                <h3 class="text-xl md:text-2xl font-semibold text-gray-800 mb-4 text-center">Table 2: Performance on RM-Bench</h3>
                <img src="{{ '/assets/images/table2_rm_bench_results.jpg' | relative_url }}" 
                     alt="Table 2: Accuracies (%) of reward models on RM-Bench"
                     class="w-full h-auto rounded-md shadow-md mx-auto border border-gray-300" style="max-width: 800px;">
            </div>

            <div class="bg-white p-4 md:p-6 rounded-xl shadow-xl border border-gray-200">
                <h3 class="text-xl md:text-2xl font-semibold text-gray-800 mb-4 text-center">Table 3: Performance on RABench (Ours)</h3>
                <img src="{{ '/assets/images/table3_rabench_results.jpg' | relative_url }}" 
                     alt="Table 3: Performance of reward models on RABench"
                     class="w-full h-auto rounded-md shadow-md mx-auto border border-gray-300" style="max-width: 900px;">
            </div>
        </div>
    </div>
</section>

<!-- Key Features -->
<section class="py-20 bg-white">
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
        <div class="text-center mb-16">
            <h2 class="text-3xl md:text-4xl font-bold text-gray-900 mb-4">Key Innovations</h2>
            <p class="text-xl text-gray-600 max-w-3xl mx-auto">
                RewardAnything introduces novel techniques for principle-following reward modeling
            </p>
        </div>

        <div class="grid grid-cols-1 md:grid-cols-2 gap-12">
            <div class="space-y-8">
                <div class="space-y-4">
                    <div class="flex items-start">
                        <div class="bg-blue-100 p-2 rounded-lg mr-4">
                            <svg class="w-5 h-5 text-blue-600" fill="currentColor" viewBox="0 0 20 20">
                                <path d="M9 12l2 2 4-4m6 2a9 9 0 11-18 0 9 9 0 0118 0z"/>
                            </svg>
                        </div>
                        <div>
                            <h4 class="font-semibold text-gray-900">Group Relative Policy Optimization (GRPO)</h4>
                            <p class="text-gray-600">Advanced RL training that learns relative preferences within response groups</p>
                        </div>
                    </div>
                    <div class="flex items-start">
                        <div class="bg-blue-100 p-2 rounded-lg mr-4">
                            <svg class="w-5 h-5 text-blue-600" fill="currentColor" viewBox="0 0 20 20">
                                <path d="M9 12l2 2 4-4m6 2a9 9 0 11-18 0 9 9 0 0118 0z"/>
                            </svg>
                        </div>
                        <div>
                            <h4 class="font-semibold text-gray-900">Listwise Evaluation</h4>
                            <p class="text-gray-600">Efficient ranking of multiple responses in a single forward pass</p>
                        </div>
                    </div>
                    <div class="flex items-start">
                        <div class="bg-blue-100 p-2 rounded-lg mr-4">
                            <svg class="w-5 h-5 text-blue-600" fill="currentColor" viewBox="0 0 20 20">
                                <path d="M9 12l2 2 4-4m6 2a9 9 0 11-18 0 9 9 0 0118 0z"/>
                            </svg>
                        </div>
                        <div>
                            <h4 class="font-semibold text-gray-900">Inference-Time Reasoning</h4>
                            <p class="text-gray-600">Explicit reasoning process for transparent decision making</p>
                        </div>
                    </div>
                </div>
                <div class="space-y-4">
                    <div class="flex items-start">
                        <div class="bg-green-100 p-2 rounded-lg mr-4">
                            <svg class="w-5 h-5 text-green-600" fill="currentColor" viewBox="0 0 20 20">
                                <path d="M9 12l2 2 4-4m6 2a9 9 0 11-18 0 9 9 0 0118 0z"/>
                            </svg>
                        </div>
                        <div>
                            <h4 class="font-semibold text-gray-900">Multi-LLM Consensus</h4>
                            <p class="text-gray-600">Ground truth from 4 state-of-the-art LLMs with algorithmic consensus</p>
                        </div>
                    </div>
                    <div class="flex items-start">
                        <div class="bg-green-100 p-2 rounded-lg mr-4">
                            <svg class="w-5 h-5 text-green-600" fill="currentColor" viewBox="0 0 20 20">
                                <path d="M9 12l2 2 4-4m6 2a9 9 0 11-18 0 9 9 0 0118 0z"/>
                            </svg>
                        </div>
                        <div>
                            <h4 class="font-semibold text-gray-900">Human Verification</h4>
                            <p class="text-gray-600">89% agreement rate with Œ∫=0.57 for reliable evaluation standards</p>
                        </div>
                    </div>
                </div>
            </div>

            <!-- RABench Description -->
            <div class="bg-gray-50 p-8 rounded-xl">
                <h3 class="text-xl font-bold text-gray-900 mb-4">RABench: Novel Evaluation Framework</h3>
                <p class="text-gray-700 mb-6">
                    We introduce RABench, a comprehensive benchmark specifically designed to evaluate reward models' 
                    ability to follow explicit natural language principles across diverse domains and criteria.
                </p>
                <div class="space-y-4">
                    <div class="flex items-center">
                        <div class="w-3 h-3 bg-blue-500 rounded-full mr-3"></div>
                        <span class="text-gray-700"><strong>1,002 validated rankings</strong> across 50 principles</span>
                    </div>
                    <div class="flex items-center">
                        <div class="w-3 h-3 bg-green-500 rounded-full mr-3"></div>
                        <span class="text-gray-700"><strong>5 principle categories:</strong> Content, Logic, Style, Tone, Structure</span>
                    </div>
                    <div class="flex items-center">
                        <div class="w-3 h-3 bg-purple-500 rounded-full mr-3"></div>
                        <span class="text-gray-700"><strong>Multiple domains:</strong> Chat, Code, Safety, Math</span>
                    </div>
                    <div class="flex items-center">
                        <div class="w-3 h-3 bg-orange-500 rounded-full mr-3"></div>
                        <span class="text-gray-700"><strong>Human-verified quality</strong> with high inter-annotator agreement</span>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<!-- Documentation -->
<section id="documentation" class="py-20 bg-gray-50">
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
        <div class="text-center mb-16">
            <h2 class="text-3xl md:text-4xl font-bold text-gray-900 mb-4">Documentation & Resources</h2>
            <p class="text-xl text-gray-600 max-w-3xl mx-auto">
                Everything you need to understand and use RewardAnything for your research and applications
            </p>
        </div>

        <div class="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-4 gap-6">
            <a href="{{ site.paper_url | default: '#' }}" class="block bg-blue-50 p-6 rounded-xl hover:bg-blue-100 transition-colors">
                <div class="text-2xl mb-3">üìÑ</div>
                <h3 class="font-semibold text-blue-900 mb-2">Research Paper</h3>
                <p class="text-sm text-blue-700">Complete methodology, experiments, and theoretical foundations</p>
            </a>

            <a href="#" class="block bg-green-50 p-6 rounded-xl hover:bg-green-100 transition-colors">
                <div class="text-2xl mb-3">üöÄ</div>
                <h3 class="font-semibold text-green-900 mb-2">API Documentation</h3>
                <p class="text-sm text-green-700">Comprehensive guide to using RewardAnything in your code</p>
            </a>

            <a href="#" class="block bg-purple-50 p-6 rounded-xl hover:bg-purple-100 transition-colors">
                <div class="text-2xl mb-3">üìä</div>
                <h3 class="font-semibold text-purple-900 mb-2">RABench Dataset</h3>
                <p class="text-sm text-purple-700">Benchmark dataset for evaluating principle-following capabilities</p>
            </a>

            <a href="{{ site.huggingface_url | default: '#' }}" class="block bg-orange-50 p-6 rounded-xl hover:bg-orange-100 transition-colors">
                <div class="text-2xl mb-3">ü§ó</div>
                <h3 class="font-semibold text-orange-900 mb-2">Model Weights</h3>
                <p class="text-sm text-orange-700">Pre-trained models ready for inference and fine-tuning</p>
            </a>
        </div>
    </div>
</section>

<!-- Citation -->
<section class="py-20 bg-gray-900 text-white">
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
        <div class="text-center mb-12">
            <h2 class="text-3xl md:text-4xl font-bold mb-4">Citation</h2>
            <p class="text-xl text-gray-300 max-w-3xl mx-auto">
                If you use RewardAnything in your research, please cite our paper
            </p>
        </div>

        <div class="max-w-4xl mx-auto">
            <div class="bg-gray-800 p-6 rounded-xl">
                <pre><code class="language-latex">@article{yu2025rewardanything,
  title={RewardAnything: Generalizable Principle-Following Reward Models},
  author={Yu, Zhuohao and Zeng, Jiali and Gu, Weizheng and Wang, Yidong and 
          Wang, Jindong and Meng, Fandong and Zhou, Jie and Zhang, Yue and 
          Zhang, Shikun and Ye, Wei},
  journal={arXiv preprint arXiv:2506.03637},
  year={2025}
}</code></pre>
            </div>
        </div>
    </div>
</section>

<style>
/* Special container for paper figures with neutral background */
.paper-figure-container {
    background: linear-gradient(135deg, #f8fafc 0%, #f1f5f9 100%);
    padding: 2rem;
    border-radius: 1rem;
    border: 1px solid #e2e8f0;
    box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.05);
}

.paper-figure-container img {
    background: white;
    padding: 1rem;
    border-radius: 0.5rem;
    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
}

.tab-button {
    @apply px-4 py-2 rounded-lg text-sm font-medium transition-colors;
    @apply bg-gray-200 text-gray-700 hover:bg-gray-300;
}

.tab-button.active {
    @apply bg-blue-600 text-white hover:bg-blue-700;
}

.tab-content {
    @apply transition-all duration-300;
}
</style> 

<script>
function selectDeploymentMethod(method) {
    // Remove selected state from all cards
    document.querySelectorAll('.deployment-card').forEach(card => {
        card.classList.remove('selected');
    });
    
    // Add selected state to clicked card
    document.querySelector(`[data-method="${method}"]`).classList.add('selected');
    
    // Show corresponding tab
    showTab(method);
}

function showTab(tabName) {
    // Hide all tab contents
    document.querySelectorAll('.tab-content').forEach(tab => {
        tab.classList.add('hidden');
    });
    
    // Show selected tab content
    document.getElementById(tabName + '-tab').classList.remove('hidden');
    
    // Trigger Prism.js to highlight code
    if (typeof Prism !== 'undefined') {
        Prism.highlightAll();
    }
}

// Initialize on page load
document.addEventListener('DOMContentLoaded', function() {
    // Select first deployment method by default WITHOUT scrolling
    document.querySelector(`[data-method="local"]`).classList.add('selected');
    showTab('local');
    
    // Highlight all code blocks
    if (typeof Prism !== 'undefined') {
        Prism.highlightAll();
    }
});
</script> 